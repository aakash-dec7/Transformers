{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "D_MODEL = 512\n",
    "MAX_LENGTH = 25\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 6\n",
    "DROPOUT = 0.25\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"eng_spn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df[\"English words/sentences\"]\n",
    "target_data = df[\"French words/sentences\"].apply(lambda x: \"<sos> \" + x + \" <eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_data)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_data)\n",
    "\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts(target_data)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "padded_input_sequences = pad_sequences(\n",
    "    input_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")\n",
    "\n",
    "padded_target_sequences = pad_sequences(\n",
    "    target_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch tensors\n",
    "input_tensor = torch.tensor(padded_input_sequences, dtype=torch.long)\n",
    "target_tensor = torch.tensor(padded_target_sequences, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader = DataLoader(\n",
    "    TensorDataset(input_tensor, target_tensor), batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Initialize positional encoding matrix with zeros\n",
    "        self.positional_encoding = torch.zeros(max_length, d_model)\n",
    "\n",
    "        # Compute positions (0 to max_length-1)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Compute scaling factor for indices\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * -(torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply sine for even indices and cosine for odd indices\n",
    "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension to positional encoding\n",
    "        self.positional_encoding = self.positional_encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_encoding[:, : x.size(1)].to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear layer to transform query(Q), key(K) and value(V)\n",
    "        self.w_Q = nn.Linear(d_model, d_model)\n",
    "        self.w_K = nn.Linear(d_model, d_model)\n",
    "        self.w_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Linear layer to combine all heads\n",
    "        self.w_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention score and normalize\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(\n",
    "            self.head_dim\n",
    "        )\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Convert scores to probabilities\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Context vector\n",
    "        context_vector = torch.matmul(attention_probs, V)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    # Split heads function\n",
    "    def split_heads(self, x):\n",
    "        batch_size, max_length, d_model = x.size()\n",
    "        x = x.view(batch_size, max_length, self.num_heads, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    # Combine heads function\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, max_length, head_dim = x.size()\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, max_length, self.d_model)\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Project input tensors into query, key and value spaces\n",
    "        Q = self.w_Q(q)\n",
    "        K = self.w_K(k)\n",
    "        V = self.w_V(v)\n",
    "\n",
    "        # Split Q, K and V into multiple heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        # Compute the context vector\n",
    "        context_vector = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine context from all heads\n",
    "        combined_head_context = self.combine_heads(context_vector)\n",
    "\n",
    "        # Project combined heads back to original d_model space\n",
    "        output = self.w_O(combined_head_context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.5):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        # Feed-Forward network\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Multihead self-attention layer\n",
    "        self.self_attention = MultiheadAttention(d_model, num_heads)\n",
    "\n",
    "        # Feed-Forward layer\n",
    "        self.ffnn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # Layer normalization layer\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        # Self attention followed by normalization and dropout\n",
    "        self_attention_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(self_attention_output))\n",
    "\n",
    "        # Feed-Forward followed by normalization and dropout\n",
    "        ffnn_output = self.ffnn(x)\n",
    "        x = self.norm2(x + self.dropout(ffnn_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Multihead self-attention layer\n",
    "        self.self_attention = MultiheadAttention(d_model, num_heads)\n",
    "\n",
    "        # Multihead cross-attention layer\n",
    "        self.cross_attention = MultiheadAttention(d_model, num_heads)\n",
    "\n",
    "        # Feed-Forward layer\n",
    "        self.ffnn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # Layer normalization layer\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "\n",
    "        # Self attention followed by normalization and dropout\n",
    "        self_attention_output = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attention_output))\n",
    "\n",
    "        # Cross attention followed by normalization and dropout\n",
    "        cross_attention_output = self.cross_attention(\n",
    "            x, encoder_output, encoder_output, src_mask\n",
    "        )\n",
    "        x = self.norm2(x + self.dropout(cross_attention_output))\n",
    "\n",
    "        # Feed-Forward followed by normalization and dropout\n",
    "        ffnn_output = self.ffnn(x)\n",
    "        x = self.norm3(x + self.dropout(ffnn_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        d_model,\n",
    "        max_length,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        d_ff,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Embedding layer for input\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # Embedding layer for target\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [Encoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [Decoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Output layer to map decoder output to target vocab size\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, input, target):\n",
    "        batch_size, target_length = target.size()\n",
    "        _, input_length = input.size()\n",
    "\n",
    "        # Create input mask (1 for non-padding, 0 for padding tokens)\n",
    "        input_mask = (input != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Padding mask for the target\n",
    "        target_padding_mask = (input != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # No peek mask for the target\n",
    "        target_no_peek_mask = torch.triu(\n",
    "            torch.ones((1, target_length, target_length)), diagonal=1\n",
    "        ).bool()\n",
    "\n",
    "        # Combine the target masks\n",
    "        target_mask = target_padding_mask & ~target_no_peek_mask\n",
    "\n",
    "        return input_mask, target_mask\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        # Generate input and target masks\n",
    "        input_mask, target_mask = self.generate_mask(input, target)\n",
    "\n",
    "        # Apply embedding\n",
    "        embedded_input = self.input_embedding(input)\n",
    "        embedded_target = self.target_embedding(target)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        input = self.positional_encoding(embedded_input)\n",
    "        target = self.positional_encoding(embedded_target)\n",
    "\n",
    "        # Pass input through encoder layers\n",
    "        for encoder_layer in self.encoder:\n",
    "            input = encoder_layer(input, input_mask)\n",
    "\n",
    "        # Pass target through decoder layers\n",
    "        for decoder_layer in self.decoder:\n",
    "            target = decoder_layer(target, input, input_mask, target_mask)\n",
    "\n",
    "        # Output layer for final prediction\n",
    "        output = self.fc_out(target)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Transformer(\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    D_MODEL,\n",
    "    MAX_LENGTH,\n",
    "    NUM_HEADS,\n",
    "    NUM_LAYERS,\n",
    "    1024,\n",
    "    DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "\n",
    "\n",
    "# Save model function\n",
    "def save_checkpoint(epoch, model, filename=\"checkpoint.pth\"):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "        },\n",
    "        filename,\n",
    "    )\n",
    "\n",
    "\n",
    "# Load model function\n",
    "def load_checkpoint(model, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "try:\n",
    "    start_epoch = load_checkpoint(model, filename=\"checkpoint.pth\")\n",
    "    print(f\"Resuming training from epoch: {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 1\n",
    "    print(f\"No checkpoint found, starting training from scratch...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Adam optimizer and Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(model, optimizer, criterion, dataloader, epochs=NUM_EPOCHS):\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        for input, target in progress_bar:\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input, target)\n",
    "\n",
    "            # Reshape output and target to calculate loss\n",
    "            output = output[:, 1:].reshape(-1, output.shape[2])  # Flatten the output\n",
    "            target = target[:, 1:].reshape(-1)  # Flatten the target\n",
    "\n",
    "            # Compute loss and backpropagation\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        total_loss += epoch_loss\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        save_checkpoint(epoch, model)\n",
    "\n",
    "    print(f\"Total Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "train(model, optimizer, criterion, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model, input_text, input_tokenizer, target_tokenizer, max_length=MAX_LENGTH\n",
    "):\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenizing and padding the input text\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([input_text])\n",
    "    padded_input_sequences = pad_sequences(\n",
    "        input_sequence, maxlen=max_length, padding=\"post\"\n",
    "    )\n",
    "    input_tensor = torch.tensor(padded_input_sequences, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Input embedding\n",
    "        embedded_input = model.input_embedding(input_tensor)\n",
    "        input_positional = model.positional_encoding(embedded_input)\n",
    "\n",
    "        # Create input mask\n",
    "        input_mask = (input_tensor != 0).unsqueeze(1).unsqueeze(2)\n",
    "        encoder_output = input_positional\n",
    "\n",
    "        # Pass input through the encoder layers\n",
    "        for encoder_layer in model.encoder:\n",
    "            encoder_output = encoder_layer(encoder_output, input_mask)\n",
    "\n",
    "    # sos and eos tokens\n",
    "    sos_token = target_tokenizer.word_index[\"sos\"]\n",
    "    eos_token = target_tokenizer.word_index[\"eos\"]\n",
    "\n",
    "    # Start decoding from the sos token\n",
    "    x_input = torch.tensor([[sos_token]], dtype=torch.long)\n",
    "\n",
    "    translated_sentence = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # Target embedding\n",
    "            embedded_target = model.target_embedding(x_input)\n",
    "            target_positional = model.positional_encoding(embedded_target)\n",
    "\n",
    "            # Create target mask\n",
    "            target_padded_mask = (x_input != 0).unsqueeze(1).unsqueeze(2)\n",
    "            target_no_peek_mask = torch.triu(\n",
    "                torch.ones(1, x_input.size(1), x_input.size(1)), diagonal=1\n",
    "            ).bool()\n",
    "\n",
    "            target_mask = target_padded_mask & target_no_peek_mask\n",
    "\n",
    "            # Pass target through the decoder layers\n",
    "            decoder_output = target_positional\n",
    "            for decoder_layer in model.decoder:\n",
    "                decoder_output = decoder_layer(\n",
    "                    decoder_output, encoder_output, input_mask, target_mask\n",
    "                )\n",
    "\n",
    "            # Get logits for the last tokens\n",
    "            logits = model.fc_out(decoder_output[:, -1, :])\n",
    "            predicted_token = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        # If predicted_token is eos, stop prediction\n",
    "        if predicted_token == eos_token:\n",
    "            break\n",
    "\n",
    "        translated_sentence.append(predicted_token)\n",
    "\n",
    "        x_input = torch.cat(\n",
    "            [x_input, torch.tensor([[predicted_token]], dtype=torch.long)], dim=1\n",
    "        )\n",
    "\n",
    "    # Convert predicted token sequence back to text\n",
    "    translated_sentence_text = target_tokenizer.sequences_to_texts(\n",
    "        [translated_sentence]\n",
    "    )[0]\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "input_sentence = \"ill teach tom\"\n",
    "translated_sentence = predict(model, input_sentence, input_tokenizer, target_tokenizer)\n",
    "print(f\"Input Sentence: {input_sentence}\")\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
