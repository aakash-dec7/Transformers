{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "D_MODEL = 512\n",
    "MAX_LENGTH = 25\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 6\n",
    "DROPOUT = 0.25\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>Spanish words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "      <td>ve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>go</td>\n",
       "      <td>vete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>go</td>\n",
       "      <td>vaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>go</td>\n",
       "      <td>vayase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hi</td>\n",
       "      <td>hola</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 English words/sentences Spanish words/sentences\n",
       "0           0                      go                      ve\n",
       "1           1                      go                    vete\n",
       "2           2                      go                    vaya\n",
       "3           3                      go                  vayase\n",
       "4           4                      hi                    hola"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"eng_spn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df[\"English words/sentences\"]\n",
    "target_data = df[\"Spanish words/sentences\"].apply(lambda x: \"<sos> \" + x + \" <eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_data)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_data)\n",
    "\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts(target_data)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "padded_input_sequences = pad_sequences(\n",
    "    input_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")\n",
    "\n",
    "padded_target_sequences = pad_sequences(\n",
    "    target_sequences, maxlen=MAX_LENGTH, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch tensors\n",
    "input_tensor = torch.tensor(padded_input_sequences, dtype=torch.long)\n",
    "target_tensor = torch.tensor(padded_target_sequences, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader = DataLoader(\n",
    "    TensorDataset(input_tensor, target_tensor), batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Initialize positional encoding matrix with zeros\n",
    "        self.positional_encoding = torch.zeros(max_length, d_model)\n",
    "\n",
    "        # Compute positions (0 to max_length-1)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Compute scaling factor for indices\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * -(torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply sine for even indices and cosine for odd indices\n",
    "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension to positional encoding\n",
    "        self.positional_encoding = self.positional_encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_encoding[:, : x.size(1)].to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear layer to transform query(Q), key(K) and value(V)\n",
    "        self.w_Q = nn.Linear(d_model, d_model)\n",
    "        self.w_K = nn.Linear(d_model, d_model)\n",
    "        self.w_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Linear layer to combine all heads\n",
    "        self.w_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention score and normalize\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(\n",
    "            self.head_dim\n",
    "        )\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Convert scores to probabilities\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Context vector\n",
    "        context_vector = torch.matmul(attention_probs, V)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    # Split heads function\n",
    "    def split_heads(self, x):\n",
    "        batch_size, max_length, d_model = x.size()\n",
    "        x = x.view(batch_size, max_length, self.num_heads, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    # Combine heads function\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, max_length, head_dim = x.size()\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, max_length, self.d_model)\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Project input tensors into query, key and value spaces\n",
    "        Q = self.w_Q(q)\n",
    "        K = self.w_K(k)\n",
    "        V = self.w_V(v)\n",
    "\n",
    "        # Split Q, K and V into multiple heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        # Compute the context vector\n",
    "        context_vector = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine context from all heads\n",
    "        combined_head_context = self.combine_heads(context_vector)\n",
    "\n",
    "        # Project combined heads back to original d_model space\n",
    "        output = self.w_O(combined_head_context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.5):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        # Feed-Forward network\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Multihead self-attention layer\n",
    "        self.self_attention = MultiheadAttention(d_model, num_heads)\n",
    "\n",
    "        # Feed-Forward layer\n",
    "        self.ffnn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # Layer normalization layer\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        # Self attention followed by normalization and dropout\n",
    "        self_attention_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(self_attention_output))\n",
    "\n",
    "        # Feed-Forward followed by normalization and dropout\n",
    "        ffnn_output = self.ffnn(x)\n",
    "        x = self.norm2(x + self.dropout(ffnn_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Multihead self-attention layer\n",
    "        self.self_attention = MultiheadAttention(d_model, num_heads)\n",
    "\n",
    "        # Multihead cross-attention layer\n",
    "        self.cross_attention = MultiheadAttention(d_model, num_heads)\n",
    "\n",
    "        # Feed-Forward layer\n",
    "        self.ffnn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # Layer normalization layer\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "\n",
    "        # Self attention followed by normalization and dropout\n",
    "        self_attention_output = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attention_output))\n",
    "\n",
    "        # Cross attention followed by normalization and dropout\n",
    "        cross_attention_output = self.cross_attention(\n",
    "            x, encoder_output, encoder_output, src_mask\n",
    "        )\n",
    "        x = self.norm2(x + self.dropout(cross_attention_output))\n",
    "\n",
    "        # Feed-Forward followed by normalization and dropout\n",
    "        ffnn_output = self.ffnn(x)\n",
    "        x = self.norm3(x + self.dropout(ffnn_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        d_model,\n",
    "        max_length,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        d_ff,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Embedding layer for input\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # Embedding layer for target\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [Encoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [Decoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Output layer to map decoder output to target vocab size\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, input, target):\n",
    "        batch_size, target_length = target.size()\n",
    "        _, input_length = input.size()\n",
    "\n",
    "        # Create input mask (1 for non-padding, 0 for padding tokens)\n",
    "        input_mask = (input != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Padding mask for the target\n",
    "        target_padding_mask = (input != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # No peek mask for the target\n",
    "        target_no_peek_mask = torch.triu(\n",
    "            torch.ones((1, target_length, target_length)), diagonal=1\n",
    "        ).bool()\n",
    "\n",
    "        # Combine the target masks\n",
    "        target_mask = target_padding_mask & ~target_no_peek_mask\n",
    "\n",
    "        return input_mask, target_mask\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        # Generate input and target masks\n",
    "        input_mask, target_mask = self.generate_mask(input, target)\n",
    "\n",
    "        # Apply embedding\n",
    "        embedded_input = self.input_embedding(input)\n",
    "        embedded_target = self.target_embedding(target)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        input = self.positional_encoding(embedded_input)\n",
    "        target = self.positional_encoding(embedded_target)\n",
    "\n",
    "        # Pass input through encoder layers\n",
    "        for encoder_layer in self.encoder:\n",
    "            input = encoder_layer(input, input_mask)\n",
    "\n",
    "        # Pass target through decoder layers\n",
    "        for decoder_layer in self.decoder:\n",
    "            target = decoder_layer(target, input, input_mask, target_mask)\n",
    "\n",
    "        # Output layer for final prediction\n",
    "        output = self.fc_out(target)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Transformer(\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    D_MODEL,\n",
    "    MAX_LENGTH,\n",
    "    NUM_HEADS,\n",
    "    NUM_LAYERS,\n",
    "    1024,\n",
    "    DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "\n",
    "\n",
    "# Save model function\n",
    "def save_checkpoint(epoch, model, filename=\"checkpoint.pth\"):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "        },\n",
    "        filename,\n",
    "    )\n",
    "\n",
    "\n",
    "# Load model function\n",
    "def load_checkpoint(model, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aks7d\\AppData\\Local\\Temp\\ipykernel_22224\\3896224334.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "try:\n",
    "    start_epoch = load_checkpoint(model, filename=\"checkpoint.pth\")\n",
    "    print(f\"Resuming training from epoch: {start_epoch}\")\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 1\n",
    "    print(f\"No checkpoint found, starting training from scratch...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Adam optimizer and Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(model, optimizer, criterion, dataloader, epochs=NUM_EPOCHS):\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        for input, target in progress_bar:\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input, target)\n",
    "\n",
    "            # Reshape output and target to calculate loss\n",
    "            output = output[:, 1:].reshape(-1, output.shape[2])  # Flatten the output\n",
    "            target = target[:, 1:].reshape(-1)  # Flatten the target\n",
    "\n",
    "            # Compute loss and backpropagation\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        total_loss += epoch_loss\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        save_checkpoint(epoch, model)\n",
    "\n",
    "    print(f\"Total Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 313/313 [05:47<00:00,  1.11s/it, loss=5.76]\n",
      "Epoch 2/50: 100%|██████████| 313/313 [05:44<00:00,  1.10s/it, loss=5.99]\n",
      "Epoch 3/50: 100%|██████████| 313/313 [05:42<00:00,  1.09s/it, loss=5.53]\n",
      "Epoch 4/50: 100%|██████████| 313/313 [06:04<00:00,  1.16s/it, loss=5.57]\n",
      "Epoch 5/50: 100%|██████████| 313/313 [05:42<00:00,  1.09s/it, loss=5.65]\n",
      "Epoch 6/50: 100%|██████████| 313/313 [05:08<00:00,  1.01it/s, loss=5.41]\n",
      "Epoch 7/50: 100%|██████████| 313/313 [05:06<00:00,  1.02it/s, loss=5.5] \n",
      "Epoch 8/50: 100%|██████████| 313/313 [05:08<00:00,  1.01it/s, loss=5.38]\n",
      "Epoch 9/50: 100%|██████████| 313/313 [05:08<00:00,  1.01it/s, loss=5.47]\n",
      "Epoch 10/50: 100%|██████████| 313/313 [05:07<00:00,  1.02it/s, loss=5.39]\n",
      "Epoch 11/50: 100%|██████████| 313/313 [05:09<00:00,  1.01it/s, loss=5.33]\n",
      "Epoch 12/50: 100%|██████████| 313/313 [05:09<00:00,  1.01it/s, loss=5.22]\n",
      "Epoch 13/50: 100%|██████████| 313/313 [05:10<00:00,  1.01it/s, loss=5.46]\n",
      "Epoch 14/50: 100%|██████████| 313/313 [05:10<00:00,  1.01it/s, loss=5.48]\n",
      "Epoch 15/50: 100%|██████████| 313/313 [05:07<00:00,  1.02it/s, loss=5.8] \n",
      "Epoch 16/50: 100%|██████████| 313/313 [05:07<00:00,  1.02it/s, loss=5.49]\n",
      "Epoch 17/50: 100%|██████████| 313/313 [05:07<00:00,  1.02it/s, loss=5.53]\n",
      "Epoch 18/50: 100%|██████████| 313/313 [05:08<00:00,  1.01it/s, loss=5.5] \n",
      "Epoch 19/50: 100%|██████████| 313/313 [05:09<00:00,  1.01it/s, loss=5.13]\n",
      "Epoch 20/50: 100%|██████████| 313/313 [05:08<00:00,  1.02it/s, loss=5.08]\n",
      "Epoch 21/50: 100%|██████████| 313/313 [05:07<00:00,  1.02it/s, loss=5.48]\n",
      "Epoch 22/50: 100%|██████████| 313/313 [05:07<00:00,  1.02it/s, loss=5.41]\n",
      "Epoch 23/50: 100%|██████████| 313/313 [05:06<00:00,  1.02it/s, loss=5.5] \n",
      "Epoch 24/50: 100%|██████████| 313/313 [05:05<00:00,  1.02it/s, loss=5.38]\n",
      "Epoch 25/50: 100%|██████████| 313/313 [05:06<00:00,  1.02it/s, loss=5.37]\n",
      "Epoch 26/50: 100%|██████████| 313/313 [05:04<00:00,  1.03it/s, loss=5.16]\n",
      "Epoch 27/50: 100%|██████████| 313/313 [05:06<00:00,  1.02it/s, loss=5.37]\n",
      "Epoch 28/50: 100%|██████████| 313/313 [05:03<00:00,  1.03it/s, loss=5.26]\n",
      "Epoch 29/50: 100%|██████████| 313/313 [05:06<00:00,  1.02it/s, loss=5.48]\n",
      "Epoch 30/50: 100%|██████████| 313/313 [05:06<00:00,  1.02it/s, loss=5.21]\n",
      "Epoch 31/50: 100%|██████████| 313/313 [05:07<00:00,  1.02it/s, loss=5.25]\n",
      "Epoch 32/50: 100%|██████████| 313/313 [05:12<00:00,  1.00it/s, loss=5.25]\n",
      "Epoch 33/50: 100%|██████████| 313/313 [05:05<00:00,  1.03it/s, loss=5.47]\n",
      "Epoch 34/50: 100%|██████████| 313/313 [05:04<00:00,  1.03it/s, loss=5.48]\n",
      "Epoch 35/50: 100%|██████████| 313/313 [05:04<00:00,  1.03it/s, loss=5.38]\n",
      "Epoch 36/50: 100%|██████████| 313/313 [05:03<00:00,  1.03it/s, loss=5.51]\n",
      "Epoch 37/50: 100%|██████████| 313/313 [05:05<00:00,  1.03it/s, loss=5.57]\n",
      "Epoch 38/50: 100%|██████████| 313/313 [05:05<00:00,  1.02it/s, loss=5.32]\n",
      "Epoch 39/50: 100%|██████████| 313/313 [05:03<00:00,  1.03it/s, loss=5.41]\n",
      "Epoch 40/50: 100%|██████████| 313/313 [05:04<00:00,  1.03it/s, loss=5.77]\n",
      "Epoch 41/50: 100%|██████████| 313/313 [05:05<00:00,  1.02it/s, loss=5.23]\n",
      "Epoch 42/50: 100%|██████████| 313/313 [05:04<00:00,  1.03it/s, loss=5.5] \n",
      "Epoch 43/50: 100%|██████████| 313/313 [05:06<00:00,  1.02it/s, loss=5.21]\n",
      "Epoch 44/50: 100%|██████████| 313/313 [05:02<00:00,  1.03it/s, loss=5.12]\n",
      "Epoch 45/50: 100%|██████████| 313/313 [05:05<00:00,  1.03it/s, loss=5.2] \n",
      "Epoch 46/50: 100%|██████████| 313/313 [05:05<00:00,  1.02it/s, loss=5.44]\n",
      "Epoch 47/50: 100%|██████████| 313/313 [05:08<00:00,  1.01it/s, loss=5.51]\n",
      "Epoch 48/50: 100%|██████████| 313/313 [05:05<00:00,  1.02it/s, loss=5.46]\n",
      "Epoch 49/50: 100%|██████████| 313/313 [05:04<00:00,  1.03it/s, loss=5.23]\n",
      "Epoch 50/50: 100%|██████████| 313/313 [05:04<00:00,  1.03it/s, loss=5.3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 269.8742544552008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train(model, optimizer, criterion, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom BLEU score implementation (token-level)\n",
    "\n",
    "\n",
    "def compute_bleu(reference, candidate, max_n=4, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1 / max_n] * max_n\n",
    "\n",
    "    # No need to split, as reference and candidate are already tokenized lists\n",
    "    reference_tokens = reference\n",
    "    candidate_tokens = candidate\n",
    "\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        # Extract n-grams for reference and candidate\n",
    "        ref_ngrams = Counter(\n",
    "            [\n",
    "                tuple(reference_tokens[i : i + n])\n",
    "                for i in range(len(reference_tokens) - n + 1)\n",
    "            ]\n",
    "        )\n",
    "        cand_ngrams = Counter(\n",
    "            [\n",
    "                tuple(candidate_tokens[i : i + n])\n",
    "                for i in range(len(candidate_tokens) - n + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        match_count = sum(min(ref_ngrams[ng], cand_ngrams[ng]) for ng in cand_ngrams)\n",
    "        total_count = max(len(candidate_tokens) - n + 1, 1)\n",
    "        precisions.append(match_count / total_count if total_count > 0 else 0)\n",
    "\n",
    "    reference_length = len(reference_tokens)\n",
    "    candidate_length = len(candidate_tokens)\n",
    "    brevity_penalty = (\n",
    "        math.exp(1 - reference_length / candidate_length)\n",
    "        if candidate_length < reference_length\n",
    "        else 1\n",
    "    )\n",
    "\n",
    "    bleu_score = brevity_penalty * math.exp(\n",
    "        sum(w * math.log(p) for w, p in zip(weights, precisions) if p > 0)\n",
    "    )\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, compute_bleu, max_n=4, weights=None):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    total_bleu_score = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for input, target in dataloader:\n",
    "\n",
    "            # Forward pass (get predictions)\n",
    "            output = model(input, target)\n",
    "\n",
    "            # Reshape output and target for comparison\n",
    "\n",
    "            output = output[:, 1:].argmax(dim=-1)  # Predicted tokens\n",
    "            target = target[:, 1:]  # Reference tokens\n",
    "\n",
    "            # Convert target and predicted tokens to lists (detach from GPU if necessary)\n",
    "            target_tokens = target.cpu().tolist()\n",
    "            predicted_tokens = output.cpu().tolist()\n",
    "\n",
    "            # Calculate BLEU score for the current batch\n",
    "            batch_bleu_score = 0\n",
    "            for ref, pred in zip(target_tokens, predicted_tokens):\n",
    "                batch_bleu_score += compute_bleu(\n",
    "                    reference=ref, candidate=pred, max_n=max_n, weights=weights\n",
    "                )\n",
    "\n",
    "            # Accumulate BLEU scores and total samples\n",
    "            total_bleu_score += batch_bleu_score\n",
    "            total_samples += len(target_tokens)\n",
    "\n",
    "    avg_bleu_score = total_bleu_score / total_samples\n",
    "\n",
    "    return avg_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4518010018049224"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, dataloader, compute_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model, input_text, input_tokenizer, target_tokenizer, max_length=MAX_LENGTH\n",
    "):\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenizing and padding the input text\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([input_text])\n",
    "    padded_input_sequences = pad_sequences(\n",
    "        input_sequence, maxlen=max_length, padding=\"post\"\n",
    "    )\n",
    "    input_tensor = torch.tensor(padded_input_sequences, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Input embedding\n",
    "        embedded_input = model.input_embedding(input_tensor)\n",
    "        input_positional = model.positional_encoding(embedded_input)\n",
    "\n",
    "        # Create input mask\n",
    "        input_mask = (input_tensor != 0).unsqueeze(1).unsqueeze(2)\n",
    "        encoder_output = input_positional\n",
    "\n",
    "        # Pass input through the encoder layers\n",
    "        for encoder_layer in model.encoder:\n",
    "            encoder_output = encoder_layer(encoder_output, input_mask)\n",
    "\n",
    "    # sos and eos tokens\n",
    "    sos_token = target_tokenizer.word_index[\"sos\"]\n",
    "    eos_token = target_tokenizer.word_index[\"eos\"]\n",
    "\n",
    "    # Start decoding from the sos token\n",
    "    x_input = torch.tensor([[sos_token]], dtype=torch.long)\n",
    "\n",
    "    translated_sentence = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # Target embedding\n",
    "            embedded_target = model.target_embedding(x_input)\n",
    "            target_positional = model.positional_encoding(embedded_target)\n",
    "\n",
    "            # Create target mask\n",
    "            target_padded_mask = (x_input != 0).unsqueeze(1).unsqueeze(2)\n",
    "            target_no_peek_mask = torch.triu(\n",
    "                torch.ones(1, x_input.size(1), x_input.size(1)), diagonal=1\n",
    "            ).bool()\n",
    "\n",
    "            target_mask = target_padded_mask & target_no_peek_mask\n",
    "\n",
    "            # Pass target through the decoder layers\n",
    "            decoder_output = target_positional\n",
    "            for decoder_layer in model.decoder:\n",
    "                decoder_output = decoder_layer(\n",
    "                    decoder_output, encoder_output, input_mask, target_mask\n",
    "                )\n",
    "\n",
    "            # Get logits for the last tokens\n",
    "            logits = model.fc_out(decoder_output[:, -1, :])\n",
    "            predicted_token = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        # If predicted_token is eos, stop prediction\n",
    "        if predicted_token == eos_token:\n",
    "            break\n",
    "\n",
    "        translated_sentence.append(predicted_token)\n",
    "\n",
    "        x_input = torch.cat(\n",
    "            [x_input, torch.tensor([[predicted_token]], dtype=torch.long)], dim=1\n",
    "        )\n",
    "\n",
    "    # Convert predicted token sequence back to text\n",
    "    translated_sentence_text = target_tokenizer.sequences_to_texts(\n",
    "        [translated_sentence]\n",
    "    )[0]\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: ill teach tom\n",
      "Translated Sentence: []\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "input_sentence = \"ill teach tom\"\n",
    "translated_sentence = predict(model, input_sentence, input_tokenizer, target_tokenizer)\n",
    "print(f\"Input Sentence: {input_sentence}\")\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
